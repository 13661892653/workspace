1、修改主机名称
vim /etc/hosts
重启

2、修改该hosts文件，添加主机跟ip的映射关系
虚拟机网络host-only
这个必须注释掉
#127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
#::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
添加以下的地址关系
192.168.1.101 COLBY-NN-101
192.168.1.102 COLBY-NN-102
192.168.1.111 COLBY-DN-111

3、安装JDK
/usr/local/jdk1.8.0_171

vi /etc/profile


export JRE_HOME=$JAVA_HOME/jre
export CLASSPATH=.:$JAVA_HOME/lib:JRE_HOME/lib

4、ssh免密码登录
三台机器分别执行：ssh-keygen -t rsa
	COLBY-NN-001
	COLBY-NN-002
	COLBY-DN-001
COLBY-NN-001 拷贝到 COLBY-NN-002、COLBY-DN-001
ssh-copy-id COLBY-NN-101
ssh-copy-id COLBY-NN-102
ssh-copy-id COLBY-DN-111


5、安装zk
cp zoo_sample.cfg zoo.cfg

mkdir /app/bigdata/zookeeper-3.4.10/tmp

在最后添加：
server.1=COLBY-NN-101:2888:3888
server.2=COLBY-NN-102:2888:3888
server.3=COLBY-DN-111:2888:3888


再创建一个空文件
touch /app/bigdata/zookeeper-3.4.10/tmp/myid
最后向该文件写入ID
echo 1 > /app/bigdata/zookeeper-3.4.10/tmp/myid

配置一下文件：
core-site.xml、hdfs-site.xml、yarn-site.xml、mapred-site.xml、hadoop-env.sh、workers 

1.3将配置好的zookeeper拷贝到其他节点(首先分别在COLBY-NN-102、COLBY-DN-111根目录下创建一个app目录：mkdir /app)
			scp -r /app/bigdata/zookeeper-3.4.10/ COLBY-NN-102:/app/bigdata/
			scp -r /app/bigdata/zookeeper-3.4.10/ COLBY-DN-111:/app/bigdata/
			
			注意：修改COLBY-NN-101、COLBY-DN-111对应/app/zookeeper-3.4.10/tmp/myid内容
			COLBY-NN-101
				echo 2 > /app/bigdata/zookeeper-3.4.10/tmp/myid
			COLBY-DN-111
				echo 3 > /app/bigdata/zookeeper-3.4.10/tmp/myid

scp -r /etc/profile COLBY-NN-102:/etc/profile
scp -r /etc/profile COLBY-DN-111:/etc/profile


scp -r  /app/bigdata/zookeeper/conf/zoo.cfg COLBY-NN-102:/app/bigdata/zookeeper/conf/
scp -r  /app/bigdata/zookeeper/conf/zoo.cfg COLBY-DN-111:/app/bigdata/zookeeper/conf/





三台服务分别执行
zkServer.sh start

执行状态
zkServer.sh status


mv  apache-flume-1.8.0-bin flume
mv apache-hive-1.2.2-bin hive
mv hadoop-2.9.0 hadoop
mv hbase-1.2.6 hbase
mv kafka_2.12-1.1.0 kafka
mv spark-2.3.0-bin-hadoop2.7 spark
mv zookeeper-3.4.10 zookeeper

拷贝hadoop文件
scp -r /app/bigdata/hadoop COLBY-NN-102:/app/bigdata/
scp -r /app/bigdata/hadoop COLBY-DN-111:/app/bigdata/


scp -r /app/bigdata/hadoop/tmp COLBY-NN-102:/app/bigdata/hadoop/


scp -r /app/bigdata/hadoop/etc/hadoop/* COLBY-NN-102:/app/bigdata/hadoop/etc/hadoop/
scp -r /app/bigdata/hadoop/etc/hadoop/* COLBY-DN-111:/app/bigdata/hadoop/etc/hadoop/


scp -r /app/bigdata/hadoop/hdfs/name/* COLBY-NN-102:/app/bigdata/hadoop/hdfs/name/

scp -r /app/bigdata/hadoop/tmp/ COLBY-NN-102:/app/bigdata/hadoop/


hadoop-daemon.sh start journalnode
hdfs zkfc –formatZK
hdfs zkfc -formatZK

hdfs namenode -format

3.1新的命令
hdfs --daemon start journalnode

hadoop-daemon.sh start journalnode


启动mysql
systemctl start mysqld
mysql -uroot -p
root/mrA)FxSK+0Fi

ALTER USER 'root'@'localhost' IDENTIFIED BY 'hadoop'; 

添加远程账户

GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'hadoop' WITH GRANT OPTION;
GRANT ALL PRIVILEGES ON *.* TO root@"172.16.16.152" IDENTIFIED BY "youpassword" WITH GRANT OPTION;
验证yarn集群 




rpm -e --nodeps jdk-1.6.0_16-fcs

stop-yarn.sh
stop-dfs.sh
start-dfs.sh
start-yarn.sh


3.1新的命令
hdfs --daemon start journalnode

（缺少用户定义而造成的）因此编辑启动和关闭

$ vim sbin/start-dfs.sh
$ vim sbin/stop-dfs.sh

HDFS_DATANODE_USER=root   
HDFS_SECONDARYNAMENODE_USER=root  

start-dfs.sh修改  追加
HDFS_NAMENODE_USER=root
HDFS_DATANODE_USER=root
HDFS_JOURNALNODE_USER=root
HDFS_ZKFC_USER=root
HDFS_SECONDARYNAMENODE_USER=root
YARN_RESOURCEMANAGER_USER=root
YARN_NODEMANAGER_USER=root



WARNING: YARN_CONF_DIR has been replaced by HADOOP_CONF_DIR



conf.Configuration: resource-types.xml not found


<property>
  <name>yarn.app.mapreduce.am.env</name>
  <value>HADOOP_MAPRED_HOME=${full path of your hadoop distribution directory}</value>
</property>
<property>
  <name>mapreduce.map.env</name>
  <value>HADOOP_MAPRED_HOME=${full path of your hadoop distribution directory}</value>
</property>
<property>
  <name>mapreduce.reduce.env</name>
  <value>HADOOP_MAPRED_HOME=${full path of your hadoop distribution directory}</value>
</property>

mkdir -p /app/bigdata/hadoop/hdfs/name
mkdir -p /app/bigdata/hadoop/hdfs/data


hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.0.jar wordcount /input /out


配置spark集群
spark-env.sh
#配置内容如下：
export SCALA_HOME=/usr/share/scala
export JAVA_HOME=/usr/java/jdk1.8.0_112/
export SPARK_MASTER_IP=master
export SPARK_WORKER_MEMORY=1g
export HADOOP_CONF_DIR=/opt/hadoop-2.7.3/etc/hadoop

slaves添加
COLBY-NN-101
COLBY-NN-102
COLBY-DN-111

分布式
scp -r  /app/bigdata/spark/ COLBY-DN-111:/app/bigdata/
scp -r  /app/bigdata/spark/ COLBY-DB-111:/app/bigdata/

启动spark集群
/app/bigdata/spark/sbin/start-all.sh
'''
	#!/bin/bash
	echo -e "\033[31m ========Start The Cluster======== \033[0m"
	echo -e "\033[31m Starting Hadoop Now !!! \033[0m"
	/opt/hadoop-2.7.3/sbin/start-all.sh
	echo -e "\033[31m Starting Spark Now !!! \033[0m"
	/opt/spark-2.1.0-bin-hadoop2.7/sbin/start-all.sh
	echo -e "\033[31m The Result Of The Command \"jps\" :  \033[0m"
	jps
	echo -e "\033[31m ========END======== \033[0m"
'''

进入spark：
[root@COLBY-NN-101 sbin]#spark-shell
2018-05-05 16:42:25 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Spark context Web UI available at http://COLBY-DN-111:4040
Spark context available as 'sc' (master = local[*], app id = local-1525509759926).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.3.0
      /_/
         
Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_171)
Type in expressions to have them evaluated.
Type :help for more information.

scala>

'''
验证spark
val file=sc.textFile("hdfs://COLBY-NN-101:9000/input/profile")
val rdd = file.flatMap(line => line.split(" ")).map(word => (word,1)).reduceByKey(_+_)
rdd.collect()
rdd.foreach(println)
'''


安装配置HBASE
hbase-env.sh
export JAVA_HOME=/usr/local/jdk（jdk安装路径）
去掉注释 # export  HBASE_MANAGES_ZK=true，使用hbase自带zookeeper。


hbase-site.xml


<configuration>
	<property>
		<name>hbase.rootdir</name> <!-- hbase存放数据目录 -->
		<value>hdfs://COLBY-NN-101:9000/opt/hbase/hbase_db</value>
		<!-- 端口要和Hadoop的fs.defaultFS端口一致-->
	</property>
	<property>
		<name>hbase.cluster.distributed</name> <!-- 是否分布式部署 -->
		<value>true</value> 
	</property>
	<property>
		<name>hbase.zookeeper.quorum</name> <!-- list of  zookooper -->
		<value>COLBY-NN-101,COLBY-NN-102,COLBY-DN-111</value> 
	</property>
	<property><!--zookooper配置、日志等的存储位置 -->
		<name>hbase.zookeeper.property.dataDir</name> 
		<value>/app/bigdata/hbase/logs/zookeeper</value>
	</property>
</configuration>

regionservers

COLBY-NN-101
COLBY-NN-102
COLBY-DN-111


分布式
scp -r  /app/bigdata/hbase/ COLBY-NN-102:/app/bigdata/
scp -r  /app/bigdata/hbase/ COLBY-DN-111:/app/bigdata/